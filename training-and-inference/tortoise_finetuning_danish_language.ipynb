{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Agpune8JjsQX",
        "sr_N55tZjFWg",
        "lYuAFtItwJgs",
        "DD1vEZp7NUna",
        "YY7JF_acNgdN",
        "Ay0tYbU8DUTo",
        "iyGVnx5jrnvV",
        "nWqmlYWL7y1V",
        "iqlrXwsB8LKC",
        "Cuzs3x_O5Hpn",
        "ThdAIrTS5OQY",
        "rGQn0eA0d0aS",
        "KG95vP2Xg44J",
        "eaNfjTicgRA8"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "2UDSAolIHAqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mount Google Drive (To save trained checkpoints and to load the dataset)"
      ],
      "metadata": {
        "id": "Agpune8JjsQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xFt4umCqjlS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone the tortoise repo"
      ],
      "metadata": {
        "id": "sr_N55tZjFWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqAlGZg0hAw3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/shovonjamali/DL-Art-School.git\n",
        "%cd DL-Art-School"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Download model weights for the VQ-VAE and Autoregressive Model (GPT-2)"
      ],
      "metadata": {
        "id": "lYuAFtItwJgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O experiments/dvae.pth\n",
        "!wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O experiments/autoregressive.pth"
      ],
      "metadata": {
        "id": "GFUUbh1CwI52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Install the requirements"
      ],
      "metadata": {
        "id": "DD1vEZp7NUna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r codes/requirements.laxed.txt"
      ],
      "metadata": {
        "id": "ch71l-rPtpg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. (OPEN THIS) Parameters to be able to train on colab"
      ],
      "metadata": {
        "id": "BQTong_qj7sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from math import ceil\n",
        "DEFAULT_TRAIN_BS = 64\n",
        "DEFAULT_VAL_BS = 32\n",
        "#@markdown # Hyperparameter calculation\n",
        "#@markdown Run this cell to obtain suggested parameters for training\n",
        "Dataset_Training_Path = \"/content/gdrive/MyDrive/tts-dataset/train.txt\" #@param {type:\"string\"}\n",
        "ValidationDataset_Training_Path = \"/content/gdrive/MyDrive/tts-dataset/val.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### **NOTE**: Dataset must be in the following format.\n",
        "\n",
        "#@markdown  `dataset/`\n",
        "#@markdown * ---├── `val.txt`\n",
        "#@markdown * ---├── `train.txt`\n",
        "#@markdown * ---├── `wavs/`\n",
        "\n",
        "#@markdown `wavs/` directory must contain `.wav` files.\n",
        "\n",
        "#@markdown  Example for `train.txt` and `val.txt`:\n",
        "\n",
        "#@markdown * `wavs/A.wav|Write the transcribed audio here.`\n",
        "\n",
        "#@markdown todo: actually check the dataset structure\n",
        "\n",
        "if Dataset_Training_Path == ValidationDataset_Training_Path:\n",
        "  print(\"WARNING: training dataset path == validation dataset path!!!\")\n",
        "  print(\"\\tThis is technically okay but will make all of the validation metrics useless. \")\n",
        "  print(\"it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.\")\n",
        "\n",
        "def txt_file_lines(p: str) -> int:\n",
        "  return len(Path(p).read_text().strip().split('\\n'))\n",
        "training_samples = txt_file_lines(Dataset_Training_Path)\n",
        "val_samples = txt_file_lines(ValidationDataset_Training_Path)\n",
        "\n",
        "if training_samples < 128: print(\"WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.\")\n",
        "if val_samples < 20: print(\"WARNING: very small validation dataset! val batch size will be scaled down to account\")\n",
        "\n",
        "def div_spillover(n: int, bs: int) -> int: # returns new batch size\n",
        "  epoch_steps,remain = divmod(n,bs)\n",
        "  if epoch_steps*2 > bs: return bs # don't bother optimising this stuff if epoch_steps are high\n",
        "  if not remain: return bs # unlikely but still\n",
        "\n",
        "  if remain*2 < bs: # \"easier\" to get rid of remainder -- should increase bs\n",
        "    target_bs = n//epoch_steps\n",
        "  else: # easier to increase epoch_steps by 1 -- decrease bs\n",
        "    target_bs = n//(epoch_steps+1)\n",
        "  assert n%target_bs < epoch_steps+2 # should be very few extra\n",
        "  return target_bs\n",
        "\n",
        "if training_samples < DEFAULT_TRAIN_BS:\n",
        "  print(\"WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway\")\n",
        "  train_bs = training_samples\n",
        "else:\n",
        "  train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)\n",
        "if val_samples < DEFAULT_VAL_BS:\n",
        "  val_bs = val_samples\n",
        "else:\n",
        "  val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)\n",
        "\n",
        "steps_per_epoch = training_samples//train_bs\n",
        "lr_decay_epochs = [20, 40, 56, 72]\n",
        "lr_decay_steps = [steps_per_epoch * e for e in lr_decay_epochs]\n",
        "print_freq = min(100, max(20, steps_per_epoch))\n",
        "val_freq = save_checkpoint_freq = print_freq * 3\n",
        "\n",
        "print(\"===CALCULATED SETTINGS===\")\n",
        "print(f'{train_bs=} {val_bs=}')\n",
        "print(f'{val_freq=} {lr_decay_steps=}')\n",
        "print(f'{print_freq=} {save_checkpoint_freq=}')"
      ],
      "metadata": {
        "id": "lQQC93cjfmNd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##_Settings for normal users:_\n",
        "Experiment_Name = \"DA-train-run-1\" #@param {type:\"string\"}\n",
        "Dataset_Training_Name= \"training_dataset\" #@param {type:\"string\"}\n",
        "ValidationDataset_Name = \"validation_dataset\" # this seems to be useless??? @param {type:\"string\"}\n",
        "SaveTrainingStates = True # @param {type:\"boolean\"}\n",
        "Keep_Last_N_Checkpoints = 1 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Resume_Training = True # @param {type:\"boolean\"}\n",
        "Training_State = 8700 # @param {type:\"integer\"}\n",
        "\n",
        "#@markdown * **NOTE**: 0 means \"keep all models saved\", which could potentially cause out-of-storage issues.\n",
        "#@markdown * Without training states, each model \"only\" takes up ~1.6GB. You should have ~50GB of free space to begin with.\n",
        "#@markdown * With training states, each model (pth+state) takes up ~4.9 GB; Colab will crash around ~10 undeleted checkpoints in this case.\n",
        "\n",
        "#@markdown ##_Other training parameters_\n",
        "Fp16 = False #@param {type:\"boolean\"}\n",
        "Use8bit = True #@param {type:\"boolean\"}\n",
        "#@markdown * **NOTE**: for some reason, fp16 does not seem to improve vram use when combined with 8bit [citation needed]. To be verified later...\n",
        "TrainingRate = \"1e-4\" #@param {type:\"string\"}\n",
        "TortoiseCompat = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown * **NOTE**: TortoiseCompat introduces some breaking changes to the training process. **If you want to reproduce older models**, disable this checkbox.\n",
        "\n",
        "#@markdown ##_Calculated settings_ override\n",
        "#@markdown #####Blank entries rely on the calculated defaults from the cell above.\n",
        "#@markdown ######**Leave them blank unless you want to adjust them manually**\n",
        "TrainBS = \"\" #@param {type:\"string\"}\n",
        "ValBS = \"\" #@param {type:\"string\"}\n",
        "ValFreq = \"\" #@param {type:\"string\"}\n",
        "LRDecaySteps = \"\" #@param {type:\"string\"}\n",
        "PrintFreq = \"\" #@param {type:\"string\"}\n",
        "SaveCheckpointFreq = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def take(orig, override):\n",
        "  if override == \"\": return orig\n",
        "  return type(orig)(override)\n",
        "\n",
        "train_bs = take(train_bs, TrainBS)\n",
        "val_bs = take(val_bs, ValBS)\n",
        "val_freq = take(val_freq, ValFreq)\n",
        "lr_decay_steps = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps\n",
        "print_freq = take(print_freq, PrintFreq)\n",
        "save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)\n",
        "assert len(lr_decay_steps) == 4\n",
        "gen_lr_steps = ', '.join(str(v) for v in lr_decay_steps)\n",
        "\n",
        "#@markdown #Run this cell after you finish editing the settings.\n",
        "\n",
        "\n",
        "%cd /content/DL-Art-School\n",
        "\n",
        "if Resume_Training != True:\n",
        "  !wget https://raw.githubusercontent.com/shovonjamali/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
        "else:\n",
        "  !wget https://raw.githubusercontent.com/shovonjamali/DL-Art-School/master/experiments/EXAMPLE_resume_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
        "  resume_path = f'../experiments/{Experiment_Name}/training_state/{Training_State}.state'\n",
        "  !sed -i 's+CHANGEME_resume_training_path+'\"$resume_path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "#@markdown This will apply the settings defined above to a fresh yml config file.\n",
        "import os\n",
        "%cd /content/DL-Art-School\n",
        "!sed -i 's/batch_size: 128/batch_size: '\"$train_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/batch_size: 64/batch_size: '\"$val_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/val_freq: 500/val_freq: '\"$val_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/500, 1000, 1400, 1800/'\"$gen_lr_steps\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/print_freq: 100/print_freq: '\"$print_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '\"$save_checkpoint_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's+CHANGEME_validation_dataset_name+'\"$ValidationDataset_Name\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_validation_dataset+'\"$ValidationDataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "if(Fp16==True):\n",
        "  os.system(\"sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml\")\n",
        "!sed -i 's/use_8bit: true/use_8bit: '\"$Use8bit\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's/disable_state_saving: true/disable_state_saving: '\"$SaveTrainingStates\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/tortoise_compat: True/tortoise_compat: '\"$TortoiseCompat\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '\"$Keep_Last_N_Checkpoints\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "!sed -i 's/CHANGEME_training_dataset_name/'\"$Dataset_Training_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/CHANGEME_your_experiment_name/'\"$Experiment_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_training_dataset+'\"$Dataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "if (not TrainingRate==\"1e-5\"):\n",
        "  os.system(\"sed -i 's+!!float 1e-5 # CHANGEME:+!!float '\" + TrainingRate + \"' #+g' ./experiments/EXAMPLE_gpt.yml\")"
      ],
      "metadata": {
        "id": "ryVeEXfxqw3n",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Reslove dependencies"
      ],
      "metadata": {
        "id": "YY7JF_acNgdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid some unwanted erros we need to downgrade the current transformer version to a specific one"
      ],
      "metadata": {
        "id": "uvXmhFBbglpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers -y"
      ],
      "metadata": {
        "id": "RN-k4dlSkz1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.29.2"
      ],
      "metadata": {
        "id": "PKs0IjmCkywH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training"
      ],
      "metadata": {
        "id": "Ay0tYbU8DUTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the below cell to delete a previous run dirs"
      ],
      "metadata": {
        "id": "w3N0CLpHxsAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DL-Art-School/experiments/Test-3-28-2024"
      ],
      "metadata": {
        "id": "EtxH8KLlqPAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DL-Art-School/experiments/Test-3-28-2024_archived_240328-111516"
      ],
      "metadata": {
        "id": "Ow3Brb83qT8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Press the stop button for this cell when you are satisfied with the results, and have seen:\n",
        "\n",
        "#@markdown `INFO:base:Saving models and training states.`\n",
        "\n",
        "#@markdown If your training run saves many models, you might exceed the storage limits on the colab runtime. To prevent this, try to delete old checkpoints in /content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/* via the file explorer panel as the training runs. **Resuming training after a crash requires config editing,** so try to not let that happen.\n",
        "\n",
        "#@markdown TODO: implement code to automatically prune useless checkpoints later && restore training states\n",
        "\n",
        "%cd /content/DL-Art-School/codes\n",
        "\n",
        "!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml"
      ],
      "metadata": {
        "id": "ju5yCN_m51r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Export to Google Drive"
      ],
      "metadata": {
        "id": "iyGVnx5jrnvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/\"):\n",
        "  print(\"Connect your Google Drive and try again.\")\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/tortoise\"):\n",
        "  os.mkdir(\"/content/gdrive/MyDrive/tortoise\")\n",
        "\n",
        "srcdir = '/content/DL-Art-School/experiments/'+Experiment_Name\n",
        "outdir = '/content/gdrive/MyDrive/tortoise/'+Experiment_Name\n",
        "if not os.path.exists(outdir):\n",
        "  os.mkdir(outdir)\n",
        "\n",
        "from pathlib import Path\n",
        "from shutil import copy, copytree\n",
        "outdir = Path(outdir)\n",
        "srcdir = Path(srcdir)\n",
        "\n",
        "#@markdown #Pick what training results you'd like to save:\n",
        "#deleteZeroStepCheckpoints = True #@param {type:\"boolean\"}\n",
        "saveLogs = True #@param {type:\"boolean\"}\n",
        "saveYml = True #@param {type:\"boolean\"}\n",
        "saveInferenceCheckpoints = True #@param {type:\"boolean\"}\n",
        "#@markdown These only need to be adjusted if you want to commit further training:\n",
        "saveEMACheckpoints = True #@param {type:\"boolean\"}\n",
        "saveTrainingStates = True #@param {type:\"boolean\"}\n",
        "\n",
        "checkpointSavingStrategy = 'minimal (last ckpt only)' #@param [\"Everything (I have infinite storage)\", \"minimal (last ckpt only)\"]\n",
        "#@markdown Note that each checkpoint takes up **1.6GB of space** -- if you want to save all checkpoints, you probably need more than 15GB of gdrive storage.\n",
        "\n",
        "\n",
        "outdir.mkdir(exist_ok=True)\n",
        "'''\n",
        "if deleteZeroStepCheckpoints:\n",
        "    for zero_model in srcdir.glob('models/0_gpt*.pth'):\n",
        "        zero_model.unlink() # remove all 0 step files; useless\n",
        "    for zero_model in srcdir.glob('training_state/0.state'):\n",
        "        zero_model.unlink()\n",
        "'''\n",
        "if saveLogs:\n",
        "    copytree(srcdir/'tb_logger', outdir/'tb_logger')\n",
        "    for log in srcdir.glob('*.log'):\n",
        "        copy(log, outdir)\n",
        "\n",
        "if saveYml:\n",
        "    for yml in srcdir.glob('*.yml'):\n",
        "        copy(yml, outdir)\n",
        "\n",
        "infer_models = list((srcdir/'models').glob('*_gpt.pth'))\n",
        "training_states = (srcdir/'training_state').iterdir()\n",
        "ema_models = (srcdir/'models').glob('*_gpt_ema.pth')\n",
        "highest_step = max(int(m.stem.split('_')[0]) for m in infer_models)\n",
        "\n",
        "def save_model_directory(glob, outsubdir):\n",
        "    outsubdir.mkdir(exist_ok=True)\n",
        "    for m in glob:\n",
        "        if checkpointSavingStrategy != 'minimal (last ckpt only)' \\\n",
        "            or int(m.stem.split('_')[0]) == highest_step:\n",
        "            copy(m, outsubdir)\n",
        "\n",
        "if saveInferenceCheckpoints:\n",
        "    save_model_directory(infer_models, outdir/'models')\n",
        "\n",
        "if saveEMACheckpoints:\n",
        "    save_model_directory(ema_models, outdir/'models')\n",
        "\n",
        "if saveTrainingStates:\n",
        "    save_model_directory(training_states, outdir/'training_state')\n",
        "\n"
      ],
      "metadata": {
        "id": "049dUor8rlxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ###If the above script does not work, use this old cell\n",
        "\n",
        "#@markdown #This will blindly copy everything to your gdrive folder.\n",
        "\n",
        "!cp -r /content/DL-Art-School/experiments/$Experiment_Name /content/gdrive/MyDrive/tortoise/"
      ],
      "metadata": {
        "id": "BqydhU4Gwlv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Continue/Resume training"
      ],
      "metadata": {
        "id": "PhUspAm8a13W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Execute the steps from 1-6 of this notebook.***"
      ],
      "metadata": {
        "id": "q0BtBPGhJGbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### High-level instructions"
      ],
      "metadata": {
        "id": "nWqmlYWL7y1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Under experiments create a new folder with the experiment name and create different sub-folders like model, training_state etc. under that. Below commands will take care of this.\n",
        "2. Uoload .gpt.yml file which was saved from the previous run into that directory (**not required, found later!**).\n",
        "![picture](https://drive.google.com/uc?id=1t59IvMKEsl04x5Xr-SgEn1o-gkRiLTZ6)\n",
        "3. After creating the sub-directories, upload/import required files which were saved after the initial run. These includes model and step. Required commands are also provided below.\n",
        "4. Uncomment the resume training path and comment the training path from the Example_gpt.yml file for resume training (**automated this stuff at step 5**).\n",
        "![picture](https://drive.google.com/uc?id=1snaLw38Sd6KVqX1DkxOVJLGeR_MQvhnt)"
      ],
      "metadata": {
        "id": "GHg5zcgc2P2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Create desired directories"
      ],
      "metadata": {
        "id": "iqlrXwsB8LKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete a previous run's direcotry (if required).\n",
        "!rm -rf /content/DL-Art-School/experiments/{Experiment_Name}\n",
        "\n",
        "# Create directories under 'experiments' as resume training requires certain folder structure.\n",
        "%cd /content/DL-Art-School/experiments\n",
        "\n",
        "# Create models folder\n",
        "!mkdir -p /content/DL-Art-School/experiments/{Experiment_Name}/models\n",
        "\n",
        "# Create tb_logger folder\n",
        "!mkdir -p /content/DL-Art-School/experiments/{Experiment_Name}/tb_logger\n",
        "\n",
        "# Create training_state folder\n",
        "!mkdir -p /content/DL-Art-School/experiments/{Experiment_Name}/training_state\n",
        "\n",
        "# Create val_images folder\n",
        "!mkdir -p /content/DL-Art-School/experiments/{Experiment_Name}/val_images"
      ],
      "metadata": {
        "id": "24LzRo-TeX1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip model file (not using this one anymore, importing the model from the google drive)\n",
        "# !unzip -u \"/content/DL-Art-School/experiments/{experiment_name}/models/480.zip\" -d \"/content/DL-Art-School/experiments/{experiment_name}/models\""
      ],
      "metadata": {
        "id": "zlhSxKwyth19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Import model and step from the drive"
      ],
      "metadata": {
        "id": "1cRGwyWgccJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions"
      ],
      "metadata": {
        "id": "Cuzs3x_O5Hpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below image shows how can we get id of the model or step from the drive."
      ],
      "metadata": {
        "id": "Mg09KNXoMSlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1vHQ-JpTY5CuZMLYS8dSr-EeiWsTS3oKZ)"
      ],
      "metadata": {
        "id": "cfwRIiKeMLjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Go to root and upgrade gdown"
      ],
      "metadata": {
        "id": "ThdAIrTS5OQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to go to the base directory first. Execute the below cell untill we are at the root/base of our current runtime"
      ],
      "metadata": {
        "id": "CXH3of9Dchx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "%cd ..\n",
        "# Upgrade the current version of gdown, which gives permission denined error while importing model or step from the drive\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "r8Q9QcFb1KrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check we are at the root/base (optional)"
      ],
      "metadata": {
        "id": "rGQn0eA0d0aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "FNWtnmmc5dPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "xgJj1_JK5fi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the model and training state"
      ],
      "metadata": {
        "id": "Az4EB7qxKA-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_ID= \"1-IlMOAvY4hsb-49MK_6KEzzD-SBmpteu\" #@param {type:\"string\"}\n",
        "Training_State_ID = \"1-Nowq8V2dr61KZpUmUK9CsfQZbPanjGN\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fl0FmVqO6VsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the models folder to import the desired model saved from previous run\n",
        "%cd /content/DL-Art-School/experiments/{Experiment_Name}/models\n",
        "# Import the model\n",
        "!gdown --id $Model_ID\n",
        "\n",
        "# Import the state\n",
        "%cd /content/DL-Art-School/experiments/{Experiment_Name}/training_state\n",
        "!gdown --id $Training_State_ID"
      ],
      "metadata": {
        "id": "Y7_UB5dLzAuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Reviewing (tried tensorboard, not working)"
      ],
      "metadata": {
        "id": "KG95vP2Xg44J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tb-nightly tensorboardX tensorboard"
      ],
      "metadata": {
        "id": "NKvZ2i65isj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "dJLKOMqei0_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "VmcflVYsg9j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "logdir_path = \"/content/DL-Art-School/experiments/Test2/tb_logger\"  # Replace with your log directory path\n",
        "print(os.listdir(logdir_path))"
      ],
      "metadata": {
        "id": "B7uY9AvGrAb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def find_latest_logfile(logdir_path):\n",
        "    # Get list of event files in the log directory\n",
        "    event_files = [f for f in os.listdir(logdir_path) if f.startswith('events.out.tfevents.')]\n",
        "    if not event_files:\n",
        "        return None  # No event files found\n",
        "\n",
        "    # Sort event files by modification time\n",
        "    event_files.sort(key=lambda x: os.path.getmtime(os.path.join(logdir_path, x)), reverse=True)\n",
        "\n",
        "    # Return the path to the latest event file\n",
        "    return os.path.join(logdir_path, event_files[0])\n",
        "\n",
        "# Function to start TensorBoard in the background\n",
        "def start_tensorboard(logdir_path):\n",
        "    latest_logfile = find_latest_logfile(logdir_path)\n",
        "    if latest_logfile is None:\n",
        "        print(\"No log files found. TensorBoard cannot be started.\")\n",
        "        return\n",
        "\n",
        "    # Start TensorBoard in the background\n",
        "    tensorboard_proc = subprocess.Popen([\"tensorboard\", \"--logdir\", latest_logfile])\n",
        "\n",
        "    # Continuously monitor the log directory for changes\n",
        "    while True:\n",
        "        latest_logfile = find_latest_logfile(logdir_path)\n",
        "        if latest_logfile is not None and latest_logfile != tensorboard_proc.args[2]:\n",
        "            tensorboard_proc.terminate()\n",
        "            tensorboard_proc.wait()\n",
        "            tensorboard_proc = subprocess.Popen([\"tensorboard\", \"--logdir\", latest_logfile])\n",
        "        time.sleep(10)  # Check for changes every 10 seconds\n",
        "\n",
        "# Specify the path to your log directory\n",
        "logdir_path = \"/content/DL-Art-School/experiments/Test2/tb_logger\"\n",
        "\n",
        "# Start TensorBoard in the background\n",
        "start_tensorboard(logdir_path)"
      ],
      "metadata": {
        "id": "2fLS6JrlrcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/DL-Art-School/experiments/Test2/tb_logger/events.out.tfevents.1710379384.e3e44fc5ce54.64171.0"
      ],
      "metadata": {
        "id": "fXPpcJ_Fg-QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Resume training"
      ],
      "metadata": {
        "id": "pPMvwFX0f2D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Press the stop button for this cell when you are satisfied with the results, and have seen:\n",
        "\n",
        "#@markdown `INFO:base:Saving models and training states.`\n",
        "\n",
        "#@markdown If your training run saves many models, you might exceed the storage limits on the colab runtime. To prevent this, try to delete old checkpoints in /content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/* via the file explorer panel as the training runs. **Resuming training after a crash requires config editing,** so try to not let that happen.\n",
        "\n",
        "#@markdown TODO: implement code to automatically prune useless checkpoints later && restore training states\n",
        "\n",
        "%cd /content/DL-Art-School/codes\n",
        "\n",
        "!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml"
      ],
      "metadata": {
        "id": "OHG2lNzAdnLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Export to Google Drive for saving the resume training's artifacts"
      ],
      "metadata": {
        "id": "eaNfjTicgRA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/\"):\n",
        "  print(\"Connect your Google Drive and try again.\")\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/tortoise\"):\n",
        "  os.mkdir(\"/content/gdrive/MyDrive/tortoise\")\n",
        "\n",
        "srcdir = '/content/DL-Art-School/experiments/'+Experiment_Name\n",
        "outdir = '/content/gdrive/MyDrive/tortoise/'+Experiment_Name\n",
        "if not os.path.exists(outdir):\n",
        "  os.mkdir(outdir)\n",
        "\n",
        "from pathlib import Path\n",
        "from shutil import copy, copytree\n",
        "outdir = Path(outdir)\n",
        "srcdir = Path(srcdir)\n",
        "\n",
        "#@markdown #Pick what training results you'd like to save:\n",
        "#deleteZeroStepCheckpoints = True #@param {type:\"boolean\"}\n",
        "saveLogs = True #@param {type:\"boolean\"}\n",
        "saveYml = True #@param {type:\"boolean\"}\n",
        "saveInferenceCheckpoints = True #@param {type:\"boolean\"}\n",
        "#@markdown These only need to be adjusted if you want to commit further training:\n",
        "saveEMACheckpoints = True #@param {type:\"boolean\"}\n",
        "saveTrainingStates = True #@param {type:\"boolean\"}\n",
        "\n",
        "checkpointSavingStrategy = 'minimal (last ckpt only)' #@param [\"Everything (I have infinite storage)\", \"minimal (last ckpt only)\"]\n",
        "#@markdown Note that each checkpoint takes up **1.6GB of space** -- if you want to save all checkpoints, you probably need more than 15GB of gdrive storage.\n",
        "\n",
        "\n",
        "outdir.mkdir(exist_ok=True)\n",
        "'''\n",
        "if deleteZeroStepCheckpoints:\n",
        "    for zero_model in srcdir.glob('models/0_gpt*.pth'):\n",
        "        zero_model.unlink() # remove all 0 step files; useless\n",
        "    for zero_model in srcdir.glob('training_state/0.state'):\n",
        "        zero_model.unlink()\n",
        "'''\n",
        "if saveLogs:\n",
        "    copytree(srcdir/'tb_logger', outdir/'tb_logger')\n",
        "    for log in srcdir.glob('*.log'):\n",
        "        copy(log, outdir)\n",
        "\n",
        "if saveYml:\n",
        "    for yml in srcdir.glob('*.yml'):\n",
        "        copy(yml, outdir)\n",
        "\n",
        "infer_models = list((srcdir/'models').glob('*_gpt.pth'))\n",
        "training_states = (srcdir/'training_state').iterdir()\n",
        "ema_models = (srcdir/'models').glob('*_gpt_ema.pth')\n",
        "highest_step = max(int(m.stem.split('_')[0]) for m in infer_models)\n",
        "\n",
        "def save_model_directory(glob, outsubdir):\n",
        "    outsubdir.mkdir(exist_ok=True)\n",
        "    for m in glob:\n",
        "        if checkpointSavingStrategy != 'minimal (last ckpt only)' \\\n",
        "            or int(m.stem.split('_')[0]) == highest_step:\n",
        "            copy(m, outsubdir)\n",
        "\n",
        "if saveInferenceCheckpoints:\n",
        "    save_model_directory(infer_models, outdir/'models')\n",
        "\n",
        "if saveEMACheckpoints:\n",
        "    save_model_directory(ema_models, outdir/'models')\n",
        "\n",
        "if saveTrainingStates:\n",
        "    save_model_directory(training_states, outdir/'training_state')"
      ],
      "metadata": {
        "id": "FvMqQQWYgRmB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}